---
title: "ACT6100 - TP"
author: "Chan Edmen, Khechman Ayman"
date: '2020-04-26'
output:
  html_document:
    df_print: paged
  pdf_document: default
fontsize: 14pt
Team: Akatutski
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=8, fig.path='Figs/',
                       warning=FALSE)
```

## 0.1 Installer les packages nécessaires
```{r install,include=FALSE}
install.packages("tidyverse",repos = "http://cran.us.r-project.org")
install.packages("mltools",repos = "http://cran.us.r-project.org")
install.packages("viridis",repos = "http://cran.us.r-project.org")
install.packages("randomForest",repos = "http://cran.us.r-project.org")
install.packages("corrr",repos = "http://cran.us.r-project.org")
install.packages("extrafont",repos = "http://cran.us.r-project.org")
install.packages('glmnet',repos = "http://cran.us.r-project.org")
install.packages("caret",repos = "http://cran.us.r-project.org")
install.packages("cluster",repos = "http://cran.us.r-project.org")
install.packages("fishualize",repos = "http://cran.us.r-project.org")
```

## 0.2 Charger les packages

```{r Charger,include=FALSE}
library("tidyverse")
library("mltools")
library("viridis")
library("randomForest")
library("corrr")
library("extrafont")
library('glmnet')
library("caret")
library("cluster")
library("fishualize") 

theme_set(theme_minimal())
```

# Partie 1 : Statistique descriptive

On télécharge la base de données

```{r Data}
data <- read_csv("https://raw.githubusercontent.com/nmeraihi/ACT6100/8f29c17d9ddd25713585fad6d1dc2296a05aa71b/TP/donnees_tp.csv")%>%
  filter(!is.na(nb_sinistre))

liste <- names(data)[str_detect(names(data),"variable")]

```

## 1.1 Corrélation entre nos variables

Vérifions la corrélation entre la variable habitudes et le nombre de sinistres.

```{r Correlation , fig.width=8 , fig.height=8 , warning=FALSE}
data %>%
  select(nb_sinistre,starts_with("variable"))%>%
  correlate() %>%
  filter(!is.na(nb_sinistre))%>%
  select(nb_sinistre,rowname)%>%
  mutate(rowname = fct_reorder(rowname,nb_sinistre))%>%
  ggplot(mapping = aes(x = rowname , y = nb_sinistre,fill = rowname))+
  geom_col()+
  coord_flip()+
  theme(legend.position = "",
        plot.title = element_text(size = 16 , family = "Arial Nova Light",hjust = 0.45),
        axis.text = element_text(family = "Arial Nova Cond Light"))+
  labs(title = "Corrélation entre le NB de sinistres et les habitudes des conducteurs",x = "Habitude" , y = ' Nombre de sinistre')

```

## 1.1.2 Variables ayant une corrélation de 0

```{r Correlation_2 , warning=FALSE}
data %>%
  select(nb_sinistre,starts_with("variable"))%>%
  correlate() %>%
  filter(!is.na(nb_sinistre))%>%
  pull(rowname) ->liste_correle

liste_correle <- liste_correle[-1]
liste


variables_correlations_nulle <-liste[!liste %in% liste_correle]
```

## 1.2 Base de données finale

Voici notre base de donnée finale sans les NA et nous avons supprimé les variables avec une corrélation de 0.

```{r Data_Final}
data <- read_csv("https://raw.githubusercontent.com/nmeraihi/ACT6100/8f29c17d9ddd25713585fad6d1dc2296a05aa71b/TP/donnees_tp.csv")%>%
  filter(!is.na(nb_sinistre))%>%
  select(-variables_correlations_nulle)
```

## 1.3 Vérifier les doublons au niveau des noms 

```{r Double}
data %>%
  count(name,sort = TRUE)%>%
  filter(n>1)
```

```{r Double_1}
data %>%
  filter(name =="Adaline Parrish")

## Aucun doublons, les personnes sont tous différents
```

On remarque que certaines personnes ont le même nom, mais ce sont des personnes différentes.

## 1.4 Zone
```{r Zone}
data %>%
  mutate(zone = as.factor(zone),
         zone = fct_lump(zone,10),
         zone = fct_reorder(zone,nb_sinistre,mean))%>%
  ggplot(mapping = aes(x = zone , fill = as.factor(nb_sinistre)))+
  geom_bar(stat = "count")+
  coord_flip()+
  scale_y_log10()+
  labs(title = "Relation entre Zone et le nombre de sinistres" , x="Zone" , y ="" , fill = "Nombre de sinistre")
```

## 1.5 Statut Marital

```{r Statut_Marital}
data %>%
  mutate(statut_marital = as.factor(statut_marital))%>%
  ggplot(mapping = aes(x = statut_marital,fill = as.factor(nb_sinistre)))+
  geom_bar(stat = "count")+
  scale_y_log10()+
  labs(title = "Relation entre le Statut Marital et le nombre sinistres" , fill = "Nombre de sinistre", x = "Statut Marital" , y ="")
```


## 1.6 Créer une nouvelle colonne

Cette colonne nous aidera à diviser le problème de prédiction en deux, ceux qui ont fait aucun accident auront un nombre de sinistre égale à 0 et ceux qui ont fait des accidents, on fera une regression pour prédire leur nombre de sinistres.


```{r Colonne}
data <- data %>%
  mutate(nb_statut = ifelse(nb_sinistre == 0 , 0 , 1 ))
```


Voyons quelles sont les variables qui déterminent si un conducteur fera un accident ou non.

#### Feature enginneering

Nous allons ajouter de nouvelles variables afin que de nous aider à prédire le nombre de sinistres.

```{r Colonne_1_1}

data %>%
  mutate(feature_1 = nb_klms_declare_annee - distance_conduite,
         feature_2 = nb_annees_permis - nb_ann_sans_accidents,
         feature_3 = nb_ann_sans_accidents*utilisation_vehicule) ->data_pred

```

```{r Statut_Marital_2 , warning=FALSE}
##Nombre de Statut
data %>%
  ggplot(mapping = aes(x = age_aconducteur , fill =as.factor(nb_statut)))+
  geom_histogram()+
  scale_y_log10()+
    labs(title = "Relation entre l'âge du conduteur et la variable accident" , fill = "Fait un accident:" , x  ="Age du conducteur" , y= "")

```

Regardons maintenant la distribution de l'âge des conducteurs en fonction de leur sexe.

```{r age}
data %>%
  ggplot(mapping = aes(x = age_aconducteur,fill = as.factor(sexe)))+
  geom_density()+
  facet_wrap(~sexe)+
  labs(title = "Distribution de l'âge des conducteurs en fonction de leur sexe" , fill = "Sexe" , x  ="Age du conducteur" , y= "Densité")
```

## 1.7 Utilisation du véhicule

Jetons un coup d'oeil sur la variable Année Vehicule

```{r Anne_Veh , warning=FALSE}
data %>%
    mutate(utilisation_vehicule = 5 * (utilisation_vehicule %/% 5))%>%
    group_by(utilisation_vehicule)%>%
    mutate(mean_sinistre = mean(nb_sinistre))%>%
    ggplot(mapping = aes(x = utilisation_vehicule , y = mean_sinistre,color = mean_sinistre))+
    geom_point(size = 1.5)+
    geom_line(size = 1.2)+
    scale_color_viridis(direction = -1  ,option = "F")+
    labs(y = "Moyenne de nombre de sinistre" , title = "Nombre d'année du véhicule par dizaine d'année" , x = "Anneé en dizaine")
  
```

Ce graphique explique deux phénomènes : 

Le premier phénomène est dû au fait que les personnes qui ont peu d'expérience vont certainement avoir plus d'accidents. De plus, au fur et à mesure que l'utilisation du véhicule augmente, leur expérience augmente ce qui fait diminuer le nombre de sinsitre moyen.

Cependant, on voit dans le graphique au temps 10 ans, le nombre de sinistre moyen augmente lorsque le minimum est atteint. Ceci est dû au fait que les sinsitres peuvent être causé par d'autres conducteurs non expérimentés et aussi au fait que notre risque d'avoir un sinistre augmente avec le temps.


PS : Ou tout simplement le manque de personne qui ont utilisé leur vehicule plus que 15 ans.

## 1.8 Nombre d'année sans accident

Nous allons maintenant jeter un coup d'oeil sur cette variable, à priori elle devrait être intéressante.

```{r annee_sans_accident}
data %>%
  filter(nb_ann_sans_accidents >= 5 )%>%
  mutate(nb_ann_sans_accidents = 5 * (nb_ann_sans_accidents %/% 5))%>%
  group_by(nb_ann_sans_accidents)%>%
  mutate(moyenne_sinistre = mean(nb_sinistre))%>%
  ggplot(mapping = aes(x = nb_ann_sans_accidents , y = moyenne_sinistre))+
  geom_point()+
  geom_smooth()+
  labs(title = "Relatio entre le nombre d'années sans accident et la moyenne de sinistre" , x = "Nombre d'années sans accident" , y = "Moyenne sinistre")
```

Exactement ce qu'on avait prédit, le nombre d'accident diminue en fonction du nombre d'année sans accident des conducteurs.

Cette variable devrait être intéressante dans nos modèles.

## 1.9 Les variables sur les caractéristiques des voitures des assurés


Regardons combien de modèle nous avons dans notre base de données

```{r mode_vehi}
data %>%
  count(model_vehicule,type_vehicule,marque_vehhicule,sort = TRUE)
```
WOW , nous avons 590 modèles avec un type de véhicule différents, cette variable n'est pas intéressante. En effet, si nous utilisons un modèle à arbre de décision cette variable pourrait créer plusieurs branches différentes donc ce modèle ne sera pas très flexible.

On remarque cependant qu'on une voiture de modèle '571' et de type '6' qui est la plus conduite par nos conducteurs.

Regardons celles qui sont les plus présentes dans notre base de données.

Nous allons créer une nouvelle colonne qui regroupe le modèle du véhicule et son type.

```{r mode_vehi_2}
data %>%
  mutate(type_mod = paste0(model_vehicule,"_",type_vehicule,"_",marque_vehhicule),
         type_mod = fct_reorder(type_mod,nb_sinistre,mean))%>%
  group_by(type_mod)%>%
  mutate(nb = n())%>%
  filter(nb > 500)%>%
  ggplot(mapping = aes(x = type_mod , y = as.factor(nb_sinistre), fill = as.factor(nb_sinistre)))+
  geom_bar(stat = "identity")+
  coord_flip()+
  labs(title = "Modèle et type de véhicule en fonction du nombre de sinistre" , fill = "Nombre de sinistre", x = "Modèle et type de véhicule", y = "Nombre de sinistre")
  
```

Au sommet, nous avons le modèle 115_3_14 qui a le plus grand *taux* de sinistre.

Dans ce graphique, nous avons choisi de classer les types de véhicule et les modèles en fonction du nombre de sinistre moyen.


## 2.0 Nombre de sinistre selon les distances parcourru

```{r NbSinDis}
data %>%
  ggplot(mapping = aes(x = distance_conduite , y = nb_sinistre))+
  geom_point()+
  labs(x = "distance conduite" , y = "nombre de sinistre", title = "Nombre de sinistre en fonction de la distance conduite")


```

Comme nous pouvons constater avec le graphique ci-dessus, la distance conduite ne serait pas la principale cause des sinistres. En effet, on peut voir que les personnes ayant eu le plus de sinistres sont celles ayant conduit une petite distance. Donc, les sinistres sont plus relier à la qualité du conducteur qu'à la distance parcourrue.

## 2.1 Nombre de kilomètre

Nous allons traiter regrouper le nombre de sinistre en plusieurs catégorie.En effet, cela nous permettrait de faire des boxplo afin de mieux voir la distribution. On peut voir le chiffre "0" comme un conducteur non risqué et le chiffre 3 étant un conducteur risqué. 

```{r annee_kilometres , warning=FALSE}
data %>%
  ggplot(mapping = aes(x = as.factor(nb_sinistre), y = nb_klms_declare_annee,color = as.factor(nb_sinistre)))+
  geom_boxplot()+
  scale_y_log10()+
  labs(title = "Nombre de kilomètre déclaré en fonction du nombre de sinistres" , y = "Nombre de kilomètre déclaré" , x ="Nombre de sinistre", color = "Nombre de sinistre : ")

```

On remarque que les deux catégories qui se distinguent le plus sont ceux qui ne font pas d'accident et ceux qui ont fait 3 accidents. La médiane de ceux qui ont fait 3 accidents est assez élevé par rapport aux autres catégories.

Regardons un peu les classes : 

```{r Colonne_1}
table(data$nb_statut)
```

Nous pouvons remarquer qu'environ 5 % des personnes ont fait un accident.

Combinaisons lineaire de certaines variables 

```{r Combinaison}
data <- data[,-c(1:2)]

linear_combination <- findLinearCombos(data)


data <- data[,-linear_combination$remove]

data_pred <- data_pred[,-linear_combination$remove]

```

Transformons les variables numériques en facteur 

```{r Num}
facteurs_liste <- c("statut_marital","sexe","zone","code_territoire","type_vehicule","marque_vehhicule","model_vehicule","code_vehicule","nb_statut")

data_pred[facteurs_liste] <- lapply(data_pred[facteurs_liste], function(x) factor(x))


```


Nous allons maintenant diviser notre base données en train et test

```{r Colonne_2}
set.seed(129) ##Pour préserver l'exactitude nos réponses

sample<- sample(1:nrow(data),nrow(data)*0.7,replace = F)

data_pred <- data_pred[,-c(1:2)]


levels(data_pred$nb_statut) <- c("non","oui")


train <- data_pred[sample,]
test <- data_pred[-sample,]

```

Vérifions les proportions afin de voir si elles sont proches.

```{r Colonne_3}
prop.table(table(train$nb_statut))
prop.table(table(test$nb_statut))
```


# Partie 2 


## 1.1 Clustering 

Nous allons utiliser du clustering pour différencier les conducteurs avec sinistre des conducteurs sans sinistre dans notre base de données.

On sélectionnera seulement les variables numériques et on va "Scale" notre base de données, car les variables ne sont pas toutes sur la même échelle de mesure.

```{r Clus , warning=FALSE}
data_pred %>%
   select_if(is.numeric) %>%
  mutate_if(is.numeric,scale) ->data_pred_cluster
```


Pour le clustering, nous allons performer un K-mean. De plus, vu que cet algorithm demande qu'on donne une valeur à k. Nous allons créer une fonction qui fera du clustering sur notre base de données. Ensuite, nous allons effectuer un graphique des résultats et on utilisera la méthode "elbow".

```{r Clus_2}
kmeans_function <- function(k) {
    cluster <- kmeans(data_pred_cluster, k)
    return (cluster$tot.withinss)
}

df_kmeans <- sapply(2:40, kmeans_function)
elbow <-data.frame(k = 2:40,value = df_kmeans)


ggplot(elbow, aes(x = k, y = value)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks = seq(2, 40, by = 1))
```


On essayera avec K=20 , pour préserver les résultats on utilisera un seed. Veuillez noter que si vous utiliser cet algorithm sur une autre base de données les résultats peuvent être différent.

```{r Clus_3}
set.seed(129)
k2_clust <- kmeans(data_pred_cluster,20)

table(k2_clust$cluster)

table(data_pred$nb_statut,k2_clust$cluster)
```

COOL ! Si on regarde bien notre tableau, on voit que ceux qui ont fait des accidents sont majoritairement dans la classe *4* et *15*. 
De plus, tous les indivius qui n'ont fait aucun ne sont pas dans ces classes. 

Donc, on associera la réponse "oui" (Ayant eu un accident) à ceux de la classe *15* et *4* et "non" aux autres classes.


```{r Clus_4}
data_pred$cluster <- k2_clust$cluster

data_pred %>%
  mutate(nb_statut_pred = ifelse(cluster == 15 | cluster == 4 ,"oui","non")) ->data_pred


prop.table(table(data_pred$nb_statut_pred,data_pred$nb_statut))

```

Donc, pour les assurés auxquels nous avons accordé une valeur "non" auront automatiquement une prédiction de *0* pour le nombre de *sinistre*.


```{r visuel}
library("fishualize")

data_pred %>%
  ggplot(mapping = aes(x = as.factor(cluster) , fill = as.factor(cluster)))+
  geom_bar(stat = "count")+
  facet_wrap(~nb_statut)+
  labs(fill=NULL)+
  scale_fill_fish_d(option = "Ostracion_whitleyi")+
  guides(fill = guide_legend(reverse = TRUE , ncol = 2))+
  theme(
    panel.grid = element_blank(),
    strip.text = element_text(size = 14,color = "#78281f"),
    plot.title = element_text(hjust = 0.5,size = 16 , color = "#a04000",family = "Arial Nova")
  )+
  labs(x = "cluster" , title = "Séparation des assurés avec sinistre et sans sinistre")
```


Update Train et Test

```{r Clus_5}
train <- data_pred[sample,]
test <- data_pred[-sample,]
```


```{r Clus_6}
test %>%
  mutate(predictions_nb_sinistre = ifelse(nb_statut_pred=="non",0,NA))->test_1


test_1 %>%
    filter(is.na(predictions_nb_sinistre)) ->test_1_avec_sinistre
```


On garde notre base de données *test* avec les valeurs non prédit qui sont les personnes qui ont fait un accident ou encore les personnes de la classe *4* et *15*.


Nous allons effectuer les mêmes étapes pour train et on entrainera des modèles sur cette base de données. Puis, on prédira la base test_1_avec_sinistre.

```{r Clus_7}
train %>%
   filter(nb_sinistre>0) ->train_avec_sinistre

```


## 1.2 XGBOOST Linéaire

Tuning parameter, notez que nous avons performé plusieurs fois le modèle afin de sélectionner les meilleurs paramètres possibles.

Tout d'abord, nous allons exécuter notre premier algorithme : *xgboost linéaire*.

```{r XG}
tun_grid <- expand.grid(
  nrounds = seq(from = 200 , to = 300,by = 50),
  eta = c(0.001,0.002,0.03),
  alpha = c(0.05,0.1),
  lambda = c(0.2,0.25,0.3)
)

```


```{r XG_2 , warning=FALSE}
model_xgbL <- train(nb_sinistre ~ age_vehicule+distance_conduite+variable_75+variable_71+exposition_en_jour+variable_73+annee_vehicule+nb_ann_sans_accidents+nb_annees_permis+variable_72+zone+exposition_temps+variable_84+nb_klms_declare_annee+variable_83+feature_3+variable_81+age_aconducteur+variable_76+feature_1+feature_2+as.factor(sexe)+variable_7+variable_9+variable_77+variable_7+variable_9+utilisation_vehicule+variable_5 + variable_14 ,preProcess =c("scale","center","nzv"), data=train_avec_sinistre , method ="xgbLinear",tuneGrid = tun_grid,trControl = trainControl(method = "cv",number = 2))
```


Le modèle finale a été selectionné selon la performance du RMSE. Les paramètres qui minimisent le RMSE sont les suivants : **nrounds= 200** , **lambda=0.3** , **alpha=0.1** et **eta=0.001**

Regardons, quelles sont les variables importantes pour cet algorithme.

```{r XG_3}
importance <- varImp(model_xgbL,scale = F)

importance <- importance$importance

data.frame(variable = row.names(importance), Gini = importance$Overall) %>%
  top_n(30)%>%
  mutate(variable = fct_reorder(variable,Gini))%>%
  ggplot(mapping = aes(x = variable , y = Gini,fill = Gini ))+
  geom_bar(stat = 'identity')+
  scale_fill_viridis(direction = -1,option = 'F')+
  theme_minimal()+
  coord_flip()+
  labs(title = "Les 30 variables qui ont eu le plus d'impact sur l'algorithme de XGBoost Linear")

```

On remarque que la 4e valeur la plus importante est : *feature_1*. En effet, c'est une variable que nous avons créé.

Passons aux prédictions, nous allons vérifier la performance de notre modèle.

```{r XG_4}
predictions_xgbL <- round(predict(model_xgbL,test_1_avec_sinistre))



prop.table(table(predictions_xgbL,test_1_avec_sinistre$nb_sinistre)
)
```


Notre modèle prédit que la majorité des assurés avec sinistre ont seulement eu un sinistre durant cette année avec une précision de 95.2973%.


# XGBOOST Tree

Comme dit le nom, on perfomera maintenant un xgboost, mais sur un arbre. Voici les paramètres pour tuner notre algorithm.

```{r XGT}
grid_default <- expand.grid(
  nrounds = seq(from = 100 , to = 200,by = 50),
  max_depth = c(5,6,7,8,9,10),
  eta = c(0.003,0.004,0.05),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)
```


```{r XGT_2 , warning=FALSE}
model_xgbT <- train(nb_sinistre ~ age_vehicule+distance_conduite+variable_75+variable_71+exposition_en_jour+variable_73+
                     annee_vehicule+nb_ann_sans_accidents+nb_annees_permis+variable_72+zone+exposition_temps+variable_84+
                       nb_klms_declare_annee+variable_83+feature_3+variable_81+age_aconducteur+variable_76+feature_1+feature_2+as.factor(sexe)+variable_7 + variable_9+variable_77+variable_7+variable_9+utilisation_vehicule+variable_5 + variable_14 ,preProcess = c("scale","center","nzv"), data=train_avec_sinistre , method ="xgbTree",tuneGrid = grid_default,
                   trControl = trainControl(method = "cv",number = 2))
```

Encore une fois, le modèle a choisi les paramètres qui minimisent le RMSE et les paramètres sont : 

**nrounds=100** , **eta=0.05** , **max_depth=5** , **gamma=0** , **colsample_bytree=1** , **min_child_weight=1**  et **subsample=1**

Observons l'importance des variables pour cet algorithme

```{r XGT_4}
importance <- varImp(model_xgbT,scale = F)

importance <- importance$importance

data.frame(variable = row.names(importance), Gini = importance$Overall) %>%
  top_n(30)%>%
  mutate(variable = fct_reorder(variable,Gini))%>%
  ggplot(mapping = aes(x = variable , y = Gini,fill = Gini ))+
  geom_bar(stat = 'identity')+
  scale_fill_viridis(direction = -1,option = 'A')+
  theme_minimal()+
  coord_flip()+
  labs(title = "Les 30 variables qui ont eu le plus d'impact sur l'algorithme de XGBoost Tree")

```


Ici, on voit que notre feature_1 est en 2e position. Vérifions les prédictions jusqu'à présent.

```{r XGT_5}
predictions_xgbT <- round(predict(model_xgbT,test_1_avec_sinistre))



prop.table(table(predictions_xgbT,test_1_avec_sinistre$nb_sinistre)
)
```

On obtient les même résultats.


## 1.3 Random Forest

```{r RF , warning=FALSE}
model_rf <- train(nb_sinistre ~ age_vehicule+distance_conduite+variable_75+variable_71+exposition_en_jour+variable_73+
                     annee_vehicule+nb_ann_sans_accidents+nb_annees_permis+variable_72+zone+exposition_temps+variable_84+
                    nb_klms_declare_annee+variable_83+feature_3+variable_81+age_aconducteur+variable_76+feature_1+feature_2+as.factor(sexe)+variable_7 + variable_9+variable_77+variable_7+variable_9+utilisation_vehicule+variable_5 + variable_14 ,preProcess = c("scale","center","nzv"), data=train_avec_sinistre , method ="rf",
                   trControl = trainControl(method = "cv",number = 2))

```

L'algorithme choisira  **mtry=2**, car il minimise le RMSE.

Vérifions l'importance des variables.

```{r RF_2 , warning=FALSE}
importance <- varImp(model_rf,scale = F)

importance <- importance$importance

data.frame(variable = row.names(importance), Gini = importance$Overall) %>%
  top_n(30)%>%
  mutate(variable = fct_reorder(variable,Gini))%>%
  ggplot(mapping = aes(x = variable , y = Gini,fill = Gini ))+
  geom_bar(stat = 'identity')+
  scale_fill_viridis(direction = -1,option = 'E')+
  theme_minimal()+
  coord_flip()+
  labs(title = "Les 30 variables ont eu le plus d'impact sur l'algorithme de Random Forest")

```


```{r RF_3 , warning=FALSE}
predictions_rf <- round(predict(model_rf,test_1_avec_sinistre))

prop.table(table(predictions_rf,test_1_avec_sinistre$nb_sinistre)
)
```

Random forest ne prédit que des **1**. Ceci augmente notre précision, mais l'algorithme pourrait être moins flexible sur une autre base de données par rapport aux deux autres algorithme précédent.

Finalement, nous avons choisi l'algorithme *XGBOOST Lineaire* .

# 1.4 Prédictions Finale

```{r PF , warning=FALSE}
test_1_avec_sinistre$predictions_nb_sinistre <- predictions_xgbL
 
test_1 %>%
  mutate(predictions_nb_sinistre = ifelse(nb_statut_pred=="oui",predictions_xgbL,predictions_nb_sinistre)) ->test_finale

```


Proportion correcte de nos prédictions.

```{r PF_2 , warning=FALSE}
table(test_finale$predictions_nb_sinistre,test$nb_sinistre)
```


```{r PF_3 , warning=FALSE}
out <- prop.table(table(test_finale$predictions_nb_sinistre,test$nb_sinistre))
print(out)
```


```{r PF_4 , warning=FALSE}
Accuracy <- out[1,1] + out[2,2] + out[3,3]
print(Accuracy)
```



## Partie 3 


Vérifions pourquoi le RMSE est un mauvais prédicteur pour sélectionner un modèle et essayons de voir quel autre "metric" choisir.


```{r P3}
normaux_liste <- matrix(ncol = 10,nrow = 100)

for(i in 1:10){
normaux_liste[,i]<- rnorm(100,0,i)
}

normaux_df <- as.data.frame(normaux_liste)

normaux_df_2 <- normaux_df^2 

names(normaux_df_2)

normaux_df_2 %>%
   mutate(row_n = row_number())%>%
   pivot_longer(-row_n,names_to = "terms",values_to = "value") ->d_1
   


   d_1 %>%
   ggplot(mapping = aes(x = row_n , y = value,group = terms, color = terms))+
   geom_line()+
  expand_limits(x = 0 , y = 0)
   
  
   
```


```{r P3_2}  

normaux_df <- abs(normaux_df)
normaux_df %>%
   mutate(row_n = row_number())%>%
   pivot_longer(-row_n,names_to = "terms",values_to = "value") ->d_2
   


   d_2%>%
   ggplot(mapping = aes(x = row_n , y = value,group = terms, color = terms))+
   geom_line()
   
  
```

Ceci montre pourquoi utiliser le RMSE avec un dataset qui a une grande dispersion est une mauvaise idée . 
Ici, le *0* est la vraie *valeur* et on change la variance qui est l'erreur, en d'autres mots on suppose que l'erreur suit une loi normale.
On remarque le *RMSE* amplie les erreurs vu qu'il met au carré les erreurs, ce qui résulte un RMSE très élevé.

Ceci est différent pour le MAE.

Pour notre choix de modèle, nous allons regarder pour chaque modèle quels sont les *3 plus importantes variables* qu'ils utilisent pour prédire la variable *nb_sinistre*. 

Ensuite, on calculera *la variance* de ces 3 variables et on choisira le *minimum* de ces valeurs.

L'idée est simple, si ces 3 variables varient trop, il y a des chances que si on prend une autre base de données l'échantillon ne sera pas représentatif à notre base de données.

Cependant si on prend un très grand échantillon , il n'y aura pas de problème . 

En effet, on pourrait se demander si on utilise la fonction *sample* et on mesure la *variance* de chaque sample pour certaines variables quel sera la différence . 


On essayera de vérifier ceci tout de suite , nous allons utiliser la variable age_vehicule, car elle est présente dans les 3 modèles comme une variable importante.


```{r P3_4}
var_liste <- matrix(ncol = 1 , nrow = 20)
var_liste_2 <- matrix(ncol = 1 , nrow = 20)
var_liste_3 <- matrix(ncol = 1 , nrow = 20)
var_liste_4 <- matrix(ncol = 1 , nrow = 20)


for(i in 1:20){
  var_smple <- sample(1:nrow(data_pred),5000,replace = F)
  var_df <- data_pred[var_smple,]
  var_liste[i,1] <- var(var_df$age_vehicule)
}


for(i in 1:20){
  var_smple <- sample(1:nrow(data_pred),5000,replace = F)
  var_df <- data_pred[var_smple,]
  var_liste_2[i,1] <- var(var_df$feature_1)
}

for(i in 1:20){
  var_smple <- sample(1:nrow(data_pred),5000,replace = F)
  var_df <- data_pred[var_smple,]
  var_liste_3[i,1] <- var(var_df$distance_conduite)
}

for(i in 1:20){
  var_smple <- sample(1:nrow(data_pred),5000,replace = F)
  var_df <- data_pred[var_smple,]
  var_liste_4[i,1] <- var(var_df$variable_75)
}
```


```{r P3_5 ,fig.width=12}
var_df <- data.frame(term = paste0("s","_",1:20) , age_vehicule = var_liste[,1] ,feature_1 = var_liste_2[,1],
                     distance_conduite = var_liste_3[,1] , variable_75 = var_liste_4[,1])

var_df %>%
  pivot_longer(-term , names_to = "variable" , values_to = "valeur_variance")%>%
  ggplot(mapping = aes(x = term , y = valeur_variance , color =term , group = 1))+
  geom_line(size = 1.1)+
  facet_wrap(~variable,scales = "free")+
  labs(x = "" , y = "" , title = "Variance des variables importantes choisi par les modèles")+
  expand_limits(y = 0)+
  theme(plot.title = element_text(size = 16 , hjust = 0.5 , color  = "#1a5276"),
        legend.position = "",
        strip.background = element_rect(fill = "#d98880"),
        strip.text = element_text(size = 12))
  

```



Déjà on voit que la variable qui varie le moins est la variable *age_vehicule*, donc on choisira un des deux modèles *xgboost*. En effet, dans les deux xgboost la variable *age_vehicule* est celle qui varie le moins.


Ensuite, pour les deux xgboost entre *Linear* et *Tree*, nous allons choisir le *Linear*, car la différence est très petite.


